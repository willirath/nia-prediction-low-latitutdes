{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downlad and prepare GFS data\n",
    "\n",
    "We'll use 20m winds as they are available in, both, the analysis fields and the forecasts. We'll also download Sea-Level Pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source\n",
    "\n",
    "See: https://thredds-jumbo.unidata.ucar.edu/thredds/catalog.html\n",
    "\n",
    "We'll go for the $0.5^\\circ$ fields.\n",
    "\n",
    "Forecast URLs are of the form:\n",
    "```python\n",
    "url=(\n",
    "    \"https://thredds-jumbo.unidata.ucar.edu/thredds/\"\n",
    "    \"dodsC/grib/NCEP/GFS/Global_0p5deg/\"\n",
    "    \"GFS_Global_0p5deg_{time_stamp}.grib2\"\n",
    ")\n",
    "```\n",
    "\n",
    "Analysis URLs are of the form:\n",
    "```python\n",
    "url=(\n",
    "    \"https://thredds-jumbo.unidata.ucar.edu/thredds/\"\n",
    "    \"dodsC/grib/NCEP/GFS/Global_0p5deg_ana/TP\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "forecast_url = (\n",
    "    \"https://thredds-jumbo.unidata.ucar.edu/thredds/\"\n",
    "    \"dodsC/grib/NCEP/GFS/Global_0p5deg/\"\n",
    "    \"GFS_Global_0p5deg_{time_stamp}.grib2\"\n",
    ")\n",
    "\n",
    "analysis_url = (\n",
    "    \"https://thredds-jumbo.unidata.ucar.edu/thredds/\"\n",
    "    \"dodsC/grib/NCEP/GFS/Global_0p5deg_ana/TP\"\n",
    ")\n",
    "\n",
    "GFS_zarr_store = \"tmp_GFS.zarr\"\n",
    "\n",
    "dask_kwargs = {\"n_workers\": 4, \"threads_per_worker\": 1, \"memory_limit\": 1.5e9}\n",
    "\n",
    "input_data_chunks = {\"time\": None, \"lat\": 300, \"lon\": 300}\n",
    "output_data_chunks = {\"time\": None, \"lat\": 100, \"lon\": 100}\n",
    "\n",
    "dask_worker_config = {\n",
    "    \"distributed.worker.memory.target\": 0.80,  # target fraction to stay below\n",
    "    \"distributed.worker.memory.spill\": 0.85,  # fraction at which we spill to disk\n",
    "    \"distributed.worker.memory.pause\": 0.90,  # fraction at which we pause worker threads\n",
    "    \"distributed.worker.memory.terminate\": 0.95,  # fraction at which we terminate the worker\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech preamble\n",
    "\n",
    "Import modules and spin up Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to less agressive memory management\n",
    "dask.config.set(dask_worker_config)\n",
    "\n",
    "# start cluster and connect\n",
    "client = Client(**dask_kwargs)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to automate finding the latest forecast URL. Do this by trying to open URLs with Xarray starting from the newest possible URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_forecast_url(url=forecast_url):\n",
    "    \"\"\"Find the latest GFS forecast dataset.\"\"\"\n",
    "    now = datetime.now()\n",
    "    today = datetime(now.year, now.month, now.day)\n",
    "    tomorrow = today + timedelta(days=1)\n",
    "    for nback in range(8):\n",
    "        try_date = tomorrow - nback * timedelta(hours=6)\n",
    "        try_time_stamp = try_date.strftime(\"%Y%m%d_%H%M\")\n",
    "        try_url = url.format(time_stamp=try_time_stamp)\n",
    "        try:\n",
    "            ds = xr.open_dataset(try_url)\n",
    "            return try_url\n",
    "        except OSError as e:\n",
    "            pass\n",
    "    raise ValueError(\"Didn't find any working forecast url.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_forecast_url = get_latest_forecast_url()\n",
    "print(f\"For the forecast: {latest_forecast_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open remote datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_ds = xr.open_dataset(\n",
    "    latest_forecast_url,\n",
    "    chunks=input_data_chunks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_ds = xr.open_dataset(\n",
    "    analysis_url,\n",
    "    chunks=input_data_chunks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix dims\n",
    "\n",
    "The GFS data come with weird dimension names (`\"time1\"`, `\"time2\"`, etc.).\n",
    "We'll extract vars from the huge collection of data vars in the datasets and drop the trailing digits from dim names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gfs_field(ds, varname):\n",
    "    \"\"\"Extract var and clean up labels.\"\"\"\n",
    "    \n",
    "    # extract var\n",
    "    var = ds[varname]\n",
    "    \n",
    "    # drop digits in dim names (time1-->time, etc.)\n",
    "    var = var.rename({d: d[:-1] for d in var.dims if d[-1].isdigit()})\n",
    "    \n",
    "    # drop singleton coords\n",
    "    var = var.drop((c for c in var.coords if var.coords[c].shape == ()))\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct datasets with only the needed fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(ds):\n",
    "    ds_extracted = xr.Dataset()\n",
    "\n",
    "    ds_extracted[\"U20\"] = extract_gfs_field(\n",
    "        ds, \"u-component_of_wind_height_above_ground\"\n",
    "    ).sel(\n",
    "        height_above_ground=20, method=\"nearest\", drop=True\n",
    "    ).rename(\"U20\")\n",
    "\n",
    "    ds_extracted[\"V20\"] = extract_gfs_field(\n",
    "        ds, \"v-component_of_wind_height_above_ground\"\n",
    "    ).sel(\n",
    "        height_above_ground=20, method=\"nearest\", drop=True\n",
    "    ).rename(\"V20\")\n",
    "\n",
    "    ds_extracted[\"SLP\"] = extract_gfs_field(\n",
    "        ds, \"Pressure_surface\"\n",
    "    ).rename(\"SLP\")\n",
    "    \n",
    "    try:\n",
    "        ds_extracted[\"ocean\"] = (\n",
    "            extract_gfs_field(\n",
    "                ds, \"Land_cover_0__sea_1__land_surface\"\n",
    "            ) == 0\n",
    "        ).isel(time=0, drop=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        ds_extracted = ds_extracted.drop([\"reftime\", ])\n",
    "    except ValueError as e:\n",
    "        pass\n",
    "    \n",
    "    return ds_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_forecast_flag(ds, is_forecast=False):\n",
    "    \"\"\"Add a flag indicating if the time step is from a forecast.\"\"\"\n",
    "    ds[\"is_forecast\"] = xr.DataArray(is_forecast, ).where(~ds[\"time\"].isnull()).astype(bool)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_ds = add_forecast_flag(construct_dataset(forecast_ds), is_forecast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_ds = add_forecast_flag(construct_dataset(analysis_ds), is_forecast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stitch together analysis and forecast\n",
    "\n",
    "If overlapping, we'll use the analysis data. This is done by setting `compat=\"override\"` in `xarray.merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis_ds.coords[\"time\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast_ds.coords[\"time\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_timesteps(forecast, analysis):\n",
    "    \"\"\"Remove timesteps from forecast that are also in analysis.\"\"\"\n",
    "\n",
    "    not_redundant = sorted(list(\n",
    "        set(forecast.coords[\"time\"].data).difference(set(analysis.coords[\"time\"].data))\n",
    "    ))\n",
    "    \n",
    "    return forecast.sel(time=not_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.concat(\n",
    "    (\n",
    "        analysis_ds,\n",
    "        drop_redundant_timesteps(forecast_ds, analysis_ds)\n",
    "    ),\n",
    "    dim=\"time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop land values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.where(ds[\"ocean\"]).drop([\"ocean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure forecast flag is only time series\n",
    "\n",
    "There's some weird broadcasting from the `Dataset.where` method happening above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"is_forecast\"] = ds[\"is_forecast\"].astype(bool).isel(\n",
    "    lat=0, lon=0, drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Wind Stress\n",
    "\n",
    "We'll use a very simple bulk formula:\n",
    "\n",
    "$$\\vec{\\tau} = \\rho_a C_d \\cdot |\\vec{U}| \\vec{U}$$\n",
    "\n",
    "with $C_d=10^{-3}=const.$ and $\\rho_a=1\\frac{kg}{m^3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_windstress(U, V, C_d=1e-3, rhoa=1):\n",
    "    spd = (U ** 2 + V ** 2) ** 0.5\n",
    "    return rhoa * C_d * spd * U, rhoa * C_d * spd * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"taux\"], ds[\"tauy\"] = calculate_windstress(ds[\"U20\"], ds[\"V20\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a final look before storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.chunk(output_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds.to_zarr(GFS_zarr_store, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look\n",
    "\n",
    "We'll extract the data at the 23W, 12N Pirata location and have a look at all the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(GFS_zarr_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pirata_12n23w = ds.sel(lat=12, lon=360-23, method=\"nearest\")\n",
    "ds_pirata_12n23w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ds_pirata_12n23w[\"U20\"].hvplot.line(label=\"U20 [m/s]\", ylabel=\"wind\")\n",
    "    * ds_pirata_12n23w[\"V20\"].hvplot.line(label=\"V20 [m/s]\", ylabel=\"wind\")\n",
    "    + ds_pirata_12n23w[\"taux\"].hvplot.line(label=\"taux [N/m2]\", ylabel=\"windstress\")\n",
    "    * ds_pirata_12n23w[\"tauy\"].hvplot.line(label=\"tauy [N/m2]\", ylabel=\"windstress\")\n",
    "    + ds_pirata_12n23w[\"SLP\"].hvplot.line(label=\"SLP [Pa]\", ylabel=\"pressure\")\n",
    ").cols(1).opts(title=\"Pirata Location 12N 23W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Finished: $(date -Ins)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "See https://github.com/willirath/nia-prediction-low-latitutdes for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
